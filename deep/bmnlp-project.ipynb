{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Cloud Storage\n# from google.cloud import storage\n# storage_client = storage.Client(project='YOUR PROJECT ID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import ast\n\n# import torch\n# from tqdm.notebook import tqdm\n\n# from transformers import BertTokenizer\n# from torch.utils.data import TensorDataset\n\n# from transformers import BertForSequenceClassification","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nimport torch\nimport torch.nn as nn\nfrom transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\nfrom transformers import BertTokenizer, BertTokenizerFast, BertForSequenceClassification, BertModel\nfrom transformers import Trainer, TrainingArguments, EarlyStoppingCallback\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEVEL = 'post'\nepochs = 5\nvalid_strategy = 1\ncolumn_used = 'text'\n\n# wandb.init(project=\"bmnlp-project-{}_level-valid_strategy_{}\".format(LEVEL, valid_strategy), entity=\"andreig\")\nwandb.init(\n    project=\"bmnlp-project\", \n    name=f\"{LEVEL}_level-valid_strategy_{valid_strategy}-column_used_{column_used}\", \n    entity=\"andreig\"\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nverbose = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed: int):\n    \"\"\"\n    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n    installed).\n \n    Args:\n        seed (:obj:`int`): The seed to set.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    if is_torch_available():\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # ^^ safe to call this function even if cuda is not available\n    if is_tf_available():\n        import tensorflow as tf\n \n        tf.random.set_seed(seed)\n \nset_seed(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_map = {\n    \"Supportive\" : 0,\n    \"Indicator\" : 1,\n    \"Ideation\" : 2,\n    \"Behavior\" : 3,\n    \"Attempt\" : 4\n}\nlabels_map_inverse = {\n    0: \"Supportive\",\n    1: \"Indicator\",\n    2: \"Ideation\",\n    3: \"Behavior\",\n    4: \"Attempt\"\n}\n\nclass SuicideDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n#         self.labels = labels\n        self.labels = [labels_map[label] for label in labels]\n\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n        item[\"labels\"] = torch.tensor([self.labels[idx]])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n\n    def __init__(self, model_name, num_labels, dropout=0.5):\n\n        super(BertClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, num_labels)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_id, mask):\n\n        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        final_layer = self.relu(linear_output)\n\n        return final_layer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def metric_evaluation(results: np.array, level: str = \"post\", verbose: bool = False) -> ():\n    \"\"\"\n    Metric Evaluation function, returns accuracy, precision, recall, ordinal error (check paper) at user or post level\n    \n    :param results: numpy array of shape (test_size, 3), results[:, 0] -> user_id, results[:, 1] -> labels, results[:, 2] -> prediction,\n    :param level  : 'post' or 'user', if 'user' we group predictions by user_id and select the most voted prediction\n    :param verbose: if True display metrics before return \n    :return: (accuracy, precision, recall, ordinal_error) \n    \"\"\"\n\n    RD = lambda x: np.round(x, 3)\n    assert level in [\"post\", \"user\"], \"Level should be in ['post', 'user']\"\n\n    if level == \"user\": \n        results = pd.DataFrame(results, columns = ['user', 'labels', 'predictions'])\n        users, labels, predictions = [], [], []\n        for group_idx, group in results.groupby(\"user\"):\n            preds = group.values[:, 2]\n            values, counts = np.unique(preds, return_counts = True)\n            \n            user = group.values[:, 0][0]\n            label = group.values[:, 1][0]\n            prediction = values[np.argmax(counts)]\n            \n            users.append(user)\n            labels.append(label)\n            predictions.append(prediction)\n\n        labels      = np.array(labels)\n        predictions = np.array(predictions)\n        \n        tp = sum(labels == predictions)\n        fp = sum(labels  < predictions)\n        fn = sum(labels  > predictions)\n        oe = sum((labels - predictions) > 1)\n\n        accuracy  = RD(tp / labels.shape[0])\n        ord_error = RD(oe / labels.shape[0])\n        precision = RD(tp / (tp + fp))\n        recall    = RD(tp / (tp + fn))\n\n    else:\n        tp = sum(results[:, 1] == results[:, 2])\n        fp = sum(results[:, 1]  < results[:, 2])\n        fn = sum(results[:, 1]  > results[:, 2])\n        oe = sum((results[:, 1] - results[:, 2]) > 1)\n\n        accuracy  = RD(tp / results.shape[0])\n        ord_error = RD(oe / results.shape[0])\n        precision = RD(tp / (tp + fp))\n        recall    = RD(tp / (tp + fn))\n\n    if verbose: print(f\"[Level: {level}] Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, Ordinal Error: {ord_error}\")\n    return (accuracy, precision, recall, ord_error)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import accuracy_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n#     # calculate accuracy using sklearn's function\n#     acc = accuracy_score(labels, preds)\n\n#     print(labels)\n#     print(labels.shape)\n#     print(np.squeeze(labels))\n#     print(np.squeeze(labels).shape)\n#     print(preds)\n#     print(preds.shape)\n#     print(valid_df.user.values)\n    \n    results = np.array((valid_df.user.values, np.squeeze(labels), preds), dtype=object).T\n    accuracy, precision, recall, ord_error = metric_evaluation(results, level=LEVEL, verbose=verbose)\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'ord_error': ord_error,\n    }\n\ndef make_compute_metrics(valid_users):\n    def compute_metrics(pred):\n        labels = pred.label_ids\n        preds = pred.predictions.argmax(-1)\n    \n#         results = np.array((valid_users, np.squeeze(labels), preds), dtype=object).T\n        results = np.zeros((len(valid_users), 3))\n        results[:, 0] = valid_users\n        results[:, 1] = np.squeeze(labels)\n        results[:, 2] = preds\n        \n        accuracy, precision, recall, ord_error = metric_evaluation(results, level=LEVEL, verbose=verbose)\n        \n        return {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'ord_error': ord_error,\n        }\n\n    return compute_metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datapath = f'data/suicide_squad.csv'\n# datapath = '/kaggle/input/suicide-squad/suicide_squad.csv'\ndatapath = f\"/kaggle/input/suicide-squad-processed/suicide_{LEVEL}_preprocessed.csv\"\ndataset = pd.read_csv(datapath)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the model we gonna train, base uncased BERT\n# check text classification models here: https://huggingface.co/models?filter=text-classification\nmodel_name = \"bert-base-cased\"\n# max sequence length for each document/sentence sample\nmax_length = 512","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['user', column_used, 'label']\noof_users, oof_labels, oof_predictions = [], [], [] \nfor fold in range(5):\n    X_train = dataset[dataset[f'{valid_strategy}_fold'] != fold][columns]\n    y_train = dataset[dataset[f'{valid_strategy}_fold'] != fold]['label'].values\n\n    X_valid = dataset[dataset[f'{valid_strategy}_fold'] == fold][columns]\n    y_valid = dataset[dataset[f'{valid_strategy}_fold'] == fold]['label'].values\n\n    print(\"Label Distribution: {} => {}\".format(*np.unique(y_valid, return_counts = True)))\n    print(f\"Train Samples: {X_train.shape[0]}, Valid Sample: {X_valid.shape[0]}\")\n    print(f\"Train Users: {np.unique(X_train['user'])[:18]}\")\n    print(f\"Valid Users: {np.unique(X_valid['user'])[:18]}\")\n    print()\n    \n    sample_size = 200\n    train_texts = list(X_train[column_used].values)\n    val_texts = list(X_valid[column_used].values)\n\n    train_labels = list(X_train.label.values)\n    valid_labels = list(X_valid.label.values)\n    \n    valid_users = list(X_valid.user.values)\n\n    # load the tokenizer\n    tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n    valid_encodings = tokenizer(val_texts, truncation=True, padding=True)\n\n    # convert our tokenized data into a torch Dataset\n    train_dataset = SuicideDataset(train_encodings, train_labels)\n    valid_dataset = SuicideDataset(valid_encodings, valid_labels)\n\n    target_names = set(train_labels)\n    # load the model and pass to device\n    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(target_names)).to(device)\n\n    # metric = compute_metrics(EvalPrediction(inputs=X_valid.user.values))\n    metric = make_compute_metrics(valid_users)\n\n    if LEVEL == 'user':\n        logging_steps = 100\n        save_steps = 100\n        eval_steps = 100\n    else:\n        logging_steps = 500\n        save_steps = 500\n        eval_steps = 500\n\n    training_args = TrainingArguments(\n        output_dir='./results',          # output directory\n        num_train_epochs=epochs,              # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=16,   # batch size for evaluation\n        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n        weight_decay=0.01,               # strength of weight decay\n        logging_dir='./logs',            # directory for storing logs\n        load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n        # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n        metric_for_best_model='accuracy',\n        greater_is_better=True,\n        logging_steps=logging_steps,               # log & save weights each logging_steps\n        save_steps=save_steps,\n        eval_steps=eval_steps,\n        evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n        fp16=True,\n        report_to=\"wandb\",\n        run_name=f\"{LEVEL}_level-valid_strategy_{valid_strategy}-column_used_{column_used}\"  # name of the W&B run (optional)\n    )\n\n    trainer = Trainer(\n        model=model,                         # the instantiated Transformers model to be trained\n        args=training_args,                  # training arguments, defined above\n        train_dataset=train_dataset,         # training dataset\n        eval_dataset=valid_dataset,          # evaluation dataset\n        compute_metrics=metric,     # the callback that computes metrics of interest\n        callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n    )\n\n    # train the model\n    trainer.train()\n\n    # evaluate the current model after training\n    trainer.evaluate()\n    \n    results_valid = trainer.predict(valid_dataset)\n    y_labels = results_valid.label_ids\n    y_preds = results_valid.predictions.argmax(-1)\n    oof_users.extend(valid_users)\n    oof_labels.extend(np.squeeze(y_labels))\n    oof_predictions.extend(y_preds)\n    \n    \nall_results = np.zeros((len(oof_users), 3))\nall_results[:, 0] = oof_users\nall_results[:, 1] = oof_labels\nall_results[:, 2] = oof_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy, precision, recall, ord_error = metric_evaluation(all_results, level=LEVEL, verbose=verbose)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_json = {\n    'accuracy': accuracy,\n    'precision': precision,\n    'recall': recall,\n    'ord_error': ord_error,\n    'level': LEVEL,\n    'valid_strategy': valid_strategy,\n    'column_used': column_used,\n    'epochs': epochs,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_json","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}